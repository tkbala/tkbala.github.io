@string{ACM = {Association for Computing Machinery,}}
@inproceedings{kiziroglou2018speed,
  abbr={JP},
  title={Speed vs efficiency and storage type in portable energy systems},
  author={Kiziroglou, ME and Cowell, Martin and Thoravi Kumaravel, Balasaravanan and Boyle, DE and Evans, JW and Wright, PK and Yeatman, EM},
  booktitle={Journal of Physics: Conference Series},
  volume={1052},
  number={1},
  pages={012026},
  year={2018},
  organization={IOP Publishing},
  preview={portableenergy.png},
  doi={10.1088/1742-6596/1052/1/012026},
    url={https://iopscience.iop.org/article/10.1088/1742-6596/1052/1/012026},

}

@inproceedings{blendscape,
author = {Rajaram, Shwetha and Numan, Nels and Kumaravel, Balasaravanan Thoravi and Marquardt, Nicolai and Wilson, Andrew D},
title = {BlendScape: Enabling End-User Customization of Video-Conferencing Environments through Generative AI},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676326},
doi = {10.1145/3654777.3676326},
abstract = {Today’s video-conferencing tools support a rich range of professional and social activities, but their generic meeting environments cannot be dynamically adapted to align with distributed collaborators’ needs. To enable end-user customization, we developed BlendScape, a rendering and composition system for video-conferencing participants to tailor environments to their meeting context by leveraging AI image generation techniques. BlendScape supports flexible representations of task spaces by blending users’ physical or digital backgrounds into unified environments and implements multimodal interaction techniques to steer the generation. Through an exploratory study with 15 end-users, we investigated whether and how they would find value in using generative AI to customize video-conferencing environments. Participants envisioned using a system like BlendScape to facilitate collaborative activities in the future, but required further controls to mitigate distracting or unrealistic visual elements. We implemented scenarios to demonstrate BlendScape’s expressiveness for supporting environment design strategies from prior work and propose composition techniques to improve the quality of environments.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {40},
numpages = {19},
keywords = {end-user customization, generative AI, video-conferencing},
location = {Pittsburgh, PA, USA},
series = {UIST '24},
selected={true},
preview={blendscape.png},
}

@inproceedings{spaceblender,
author = {Numan, Nels and Rajaram, Shwetha and Kumaravel, Balasaravanan Thoravi and Marquardt, Nicolai and Wilson, Andrew D},
title = {SpaceBlender: Creating Context-Rich Collaborative Spaces Through Generative 3D Scene Blending},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676361},
doi = {10.1145/3654777.3676361},
abstract = {There is increased interest in using generative AI to create 3D spaces for Virtual Reality (VR) applications. However, today’s models produce artificial environments, falling short of supporting collaborative tasks that benefit from incorporating the user’s physical context. To generate environments that support VR telepresence, we introduce SpaceBlender, a novel pipeline that utilizes generative AI techniques to blend users’ physical surroundings into unified virtual spaces. This pipeline transforms user-provided 2D images into context-rich 3D environments through an iterative process consisting of depth estimation, mesh alignment, and diffusion-based space completion guided by geometric priors and adaptive text prompts. In a preliminary within-subjects study, where 20 participants performed a collaborative VR affinity diagramming task in pairs, we compared SpaceBlender with a generic virtual environment and a state-of-the-art scene generation framework, evaluating its ability to create virtual spaces suitable for collaboration. Participants appreciated the enhanced familiarity and context provided by SpaceBlender but also noted complexities in the generative environments that could detract from task focus. Drawing on participant feedback, we propose directions for improving the pipeline and discuss the value and design of blended spaces for different scenarios.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {41},
numpages = {25},
keywords = {VR telepresence, generative AI},
location = {Pittsburgh, PA, USA},
series = {UIST '24},
selected={true},
preview={spaceblender.jpg},
}

@inproceedings{sharednerf,
author = {Sakashita, Mose and Thoravi Kumaravel, Balasaravanan and Marquardt, Nicolai and Wilson, Andrew David},
title = {SharedNeRF: Leveraging Photorealistic and View-dependent Rendering for Real-time and Remote Collaboration},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642945},
doi = {10.1145/3613904.3642945},
abstract = {Collaborating around physical objects necessitates examining different aspects of design or hardware in detail when reviewing or inspecting physical artifacts or prototypes. When collaborators are remote, coordinating the sharing of views of their physical environment becomes challenging. Video-conferencing tools often do not provide the desired viewpoints for a remote viewer. While RGB-D cameras offer 3D views, they lack the necessary fidelity. We introduce SharedNeRF, designed to enhance synchronous remote collaboration by leveraging the photorealistic and view-dependent nature of Neural Radiance Field (NeRF). The system complements the higher visual quality of the NeRF rendering with the instantaneity of a point cloud and combines them through carefully accommodating the dynamic elements within the shared space, such as hand gestures and moving objects. The system employs a head-mounted camera for data collection, creating a volumetric task space on the fly and updating it as the task space changes. In our preliminary study, participants successfully completed a flower arrangement task, benefiting from SharedNeRF’s ability to render the space in high fidelity from various viewpoints.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {675},
numpages = {14},
keywords = {Collaboration, NeRF, Spatial Interfaces;},
location = {Honolulu, HI, USA},
series = {CHI '24},
selected={true},
preview={sharednerf.png},
}
@inproceedings{rajaram2024blendscapeenablingunifiedpersonalized,
      title={BlendScape: Enabling Unified and Personalized Video-Conferencing Environments through Generative AI}, 
      author={Shwetha Rajaram and Nels Numan and Balasaravanan Thoravi Kumaravel and Nicolai Marquardt and Andrew D. Wilson},
      year={2024},
      eprint={2403.13947},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2403.13947},
      preview={blendscape.png},
      selected={true}, 
}
@inproceedings{lyu2023streamfunnelfacilitatingcommunicationvr,
      title={StreamFunnel: Facilitating Communication Between a VR Streamer and Many Spectators}, 
      author={Haohua Lyu and Cyrus Vachha and Qianyi Chen and Balasaravanan Thoravi Kumaravel and Bjöern Hartmann},
      year={2023},
      eprint={2311.14930},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2311.14930}, 
      preview={streamfunnel.png},
      selected={true},
}
@inproceedings{kumaravel2017development,
  title={Development of an Internet of Things enabled manufacturing system for tool wear characterization},
  author={Thoravi Kumaravel, Balasaravanan and Bhattacharyya, Rahul and Siegel, Joshua and Sarma, Sanjay and Arunachalam, Narayanan},
  booktitle={2017 IEEE 3rd International Symposium in Robotics and Manufacturing Automation (ROMA)},
  pages={1--6},
  year={2017},
  organization={IEEE},
  preview={millingtool.png},
  doi={10.1109/ROMA.2017.8231733},
    url={https://ieeexplore.ieee.org/abstract/document/8231733},
    html={https://ieeexplore.ieee.org/document/8231733},
    abbr={ROMA},
    publisher={IEEE},
    pdf={millingtool.pdf},
}

@inproceedings{thoravi2019tutorivr,
  title={TutoriVR: A Video-Based Tutorial System for Design Applications in Virtual Reality},
  author={Thoravi Kumaravel, Balasaravanan and Nguyen, Cuong and DiVerdi, Stephen and Hartmann, Bjoern},
  booktitle={CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages={284--1},
  year={2019},
  selected={true},
  preview={tutorivr.jpg},
  doi={10.1145/3290605.3300514},
    url={https://dl.acm.org/doi/abs/10.1145/3290605.3300514},
    html={https://dl.acm.org/doi/fullHtml/10.1145/3290605.3300514},
    abbr={CHI},
    publisher={ACM},
    pdf={tutorivr.pdf},
}

@inproceedings{thoravi2019loki,
  title={Loki: Facilitating remote instruction of physical tasks using bi-directional mixed-reality telepresence},
  author={Thoravi Kumaravel, Balasaravanan and Anderson, Fraser and Fitzmaurice, George and Hartmann, Bjoern and Grossman, Tovi},
  booktitle={Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
  pages={161--174},
  year={2019},
  selected={true},
  preview={loki.jpg},
    doi={10.1145/3332165.3347872},
    url={https://dl.acm.org/doi/abs/10.1145/3332165.3347872},
    html={https://dl.acm.org/doi/fullHtml/10.1145/3332165.3347872},
    abbr={UIST},
    publisher={ACM},
    pdf={loki.pdf},
}

@inproceedings{claudino2020living,
  title={Living Paper: Authoring AR Narratives Across Digital and Tangible Media},
  author={Claudino Daffara, Stephanie and Brewer, Anna and Thoravi Kumaravel, Balasaravanan and Hartmann, Bjoern},
  booktitle={Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--10},
  year={2020},
  preview={livingpaper.png},
  doi={10.1145/3334480.3383091},
    url={https://dl.acm.org/doi/abs/10.1145/3334480.3383091},
    html={https://dl.acm.org/doi/fullHtml/10.1145/3334480.3383091},
    abbr={CHI},
    publisher={ACM},

}

@inproceedings{thoravi2020transceivr,
  title={TransceiVR: Bridging asymmetrical communication between VR users and external collaborators},
  author={Thoravi Kumaravel, Balasaravanan and Nguyen, Cuong and DiVerdi, Stephen and Hartmann, Bjoern},
  booktitle={Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
  pages={182--195},
  year={2020},
  selected={true},
  preview={transceivr.jpg},
  doi={10.1145/3379337.3415827},
    url={https://dl.acm.org/doi/abs/10.1145/3379337.3415827},
    html={https://dl.acm.org/doi/fullHtml/10.1145/3379337.3415827},
    abbr={UIST},
    pdf={transceivr.pdf},


}

@article{daffara2020authorive,
  title={AuthorIVE: Authoring Interactions for Virtual Environments through Disambiguating Demonstrations},
  author={Daffara, Stephanie Claudino and Saldarini, Federico and Thoravi Kumaravel, Balasaravanan and Hartmann, Bjoern},
  year={2020},
  preview={authorive.png},
  url={https://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-122.pdf},
  html={https://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-122.html},
  institution={EECS Department, UC Berkeley},


}

@techreport{pant2021model,
  title={Model-based Formalization of the Autonomy-to-Human Perception Hand-off},
  author={Pant, Yash Vardhan and Thoravi Kumaravel, Balasaravanan and Shah, Ameesh and Kraemer, Erin and Vazquez-Chanlatte, Marcell and Kulkarni, K and Hartmann, Bjoern and Seshia, Sanjit A},
  year={2021},
  institution={EECS Department, UC Berkeley},
  preview={modelformalization.png},
  url={https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-8.html},
  html={https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-8.html},
}

@inproceedings{lyu2022webtransceivr,
  title={WebTransceiVR: Asymmetrical communication between multiple VR and non-VR users online},
  author={Lyu, Haohua and Vachha, Cyrus and Chen, Qianyi and Pyrinis, Odysseus and Liou, Avery and Thoravi Kumaravel, Balasaravanan and Hartmann, Bjoern},
  booktitle={CHI Conference on Human Factors in Computing Systems Extended Abstracts},
  pages={1--7},
  year={2022},
  preview={webtransceivr.png},
  doi={10.1145/3491101.3519816},
  url={https://dl.acm.org/doi/abs/10.1145/3491101.3519816},
  html={https://dl.acm.org/doi/fullHtml/10.1145/3491101.3519816},
  abbr={CHI},
  publisher={ACM},
  pdf={webtransceivr.pdf},
    
}

@misc{kumaravel2022shaping,
  title={Shaping the new future of work through mixed reality},
  author={Kumaravel, Bala},
  journal={XRDS: Crossroads, The ACM Magazine for Students},
  volume={29},
  number={1},
  pages={8--9},
  year={2022},
  publisher={ACM New York, NY, USA},
  preview={crossroads.png},
  doi={10.1145/3558186},
    url={https://dl.acm.org/doi/abs/10.1145/3558186},
    html={https://dl.acm.org/doi/fullHtml/10.1145/3558186},
    abbr={XRDS},
    publisher={ACM},
    selected={true},
}

@phdthesis{thoravi2022interactive,
  title={Interactive Cross-Dimensional Media for Collaboration and Guidance in Mixed Reality Environments},
  author={Thoravi Kumaravel, Balasaravanan},
  year={2022},
  school={University of California, Berkeley},
  selected={true},
  preview={thesis.png},
  pdf={EECS-2022-221.pdf},
  url={https://www2.eecs.berkeley.edu/Pubs/TechRpts/2022/EECS-2022-221.html},
  html={https://www2.eecs.berkeley.edu/Pubs/TechRpts/2022/EECS-2022-221.html},
  publisher={EECS Department, University of California, Berkeley},

}

@article{thoravi2022interactive,
  title={Interactive mixed-dimensional media for cross-dimensional collaboration in mixed reality environments},
  author={Thoravi Kumaravel, Balasaravanan and Hartmann, Bjoern},
  journal={Frontiers in Virtual Reality},
  volume={3},
  pages={766336},
  year={2022},
  publisher={Frontiers},
  preview={frvir.jpg},
  pdf={frvir-03-766336.pdf},
  doi={10.3389/frvir.2022.766336},
  url={https://www.frontiersin.org/article/10.3389/frvir.2022.766336},
  publisher={Frontiers},
    abbr={FRVIR},
    html={https://www.frontiersin.org/articles/10.3389/frvir.2022.766336/full},
    pdf={frvir-03-766336.pdf},

}

@inproceedings{pant2022modeling,
  title={Modeling and Influencing Human Attentiveness in Autonomy-to-Human Perception Hand-offs},
  author={Pant, Yash Vardhan and Thoravi Kumaravel, Balasaravanan and Shah, Ameesh and Kraemer, Erin and Vazquez-Chanlatte, Marcell and Kulkarni, Kshitij and Hartmann, Bjoern and Seshia, Sanjit A},
  booktitle={2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)},
  pages={2585--2592},
  year={2022},
  organization={IEEE},
  preview={itsc.png},
  pdf={modelling_itsc.pdf},
  doi={10.1109/ITSC55140.2022.9921935},
  url={https://ieeexplore.ieee.org/abstract/document/9921935},
  abbr={ITSC},
  selected={true},
  html={https://ieeexplore.ieee.org/document/9921935},
    publisher={IEEE},
    abbrev={ITSC},
    pdf={modelling_itsc.pdf},


}

@inproceedings{thoravi2022dreamstream,
  abbr={CHI},
  title={DreamStream: Immersive and Interactive Spectating in VR},
  author={Thoravi Kumaravel, Balasaravanan and Wilson, Andrew D},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  year={2022},
  selected={true},
  preview={dreamstream.gif},
  doi={10.1145/3491102.3517508},
  url={https://dl.acm.org/doi/abs/10.1145/3491102.3517508},
  pdf={dreamstream.pdf},
  html={https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517508},
  publisher={ACM},
}

@inproceedings{alternail,
author = {Dierk, Christine and Vega G\'{a}lvez, Tom\'{a}s and Paulos, Eric},
title = {AlterNail: Ambient, Batteryless, Stateful, Dynamic Displays at Your Fingertips},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025924},
doi = {10.1145/3025453.3025924},
abstract = {Beyond phones, watches, and activity tracking devices, a new ecosystem of functional and fashionable wearable technologies can easily, safely, and economically be designed, prototyped, and integrated directly on the body. In this paper, we present AlterNail, a fingernail form factor, ambient, low-power, stateful, wireless, dynamic display with onboard vibrational sensing. AlterNail integrates a batteryless design using inductive coupling with e-ink technology to enable both quick dynamic and long-term static fingernail based visual designs without the need for power. We also detail the use of simple vibrational signals to uniquely identify everyday objects as they are handled using AlterNails. The intentionally limited interactional functionality of AlterNails, coupled with the rich personal and dynamic expressive potential, combine to present a compelling range of opportunities for designers of new interactive wearable technologies. We detail a range of practical and playful applications using this technology.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {6754–6759},
numpages = {6},
keywords = {cosmetic computing, wearables, fingernails, ambient devices},
location = {Denver, Colorado, USA},
series = {CHI '17},
hide = true
}

@article{ravi2025out,
  title={Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames},
  author={Ravi, Sahithya and Sarch, Gabriel and Vineet, Vibhav and Wilson, Andrew D and Kumaravel, Balasaravanan Thoravi},
  journal={arXiv preprint arXiv:2505.24257},
  year={2025},
  selected={true},
  pdf={https://aclanthology.org/2025.findings-acl.663.pdf}

}

@inproceedings{sarch-etal-2025-grounding,
    title = {Grounding Task Assistance with Multimodal Cues from a Single Demonstration},
    author = {Sarch, Gabriel Herbert  and Kumaravel, Balasaravanan Thoravi  and Ravi, Sahithya  and Vineet, Vibhav  and Wilson, Andrew D},
    preview={mica.png},
    booktitle = {Findings of the Association for Computational Linguistics: ACL 2025},
    month = {jul},
    year = {2025},
    address = {Vienna, Austria},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2025.findings-acl.663/},
    doi = {10.18653/v1/2025.findings-acl.663},
    pages = {12807--12833},
    ISBN = {979-8-89176-256-5},
    abstract = {A person's demonstration often serves as a key reference for others learning the same task. However, RGB video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. This sensory gap fundamentally limits the ability of Vision Language Models (VLMs) to reason about why actions occur and how they should adapt to individual users. To address this, we introduce MICA (Multimodal Interactive Contextualized Assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. Evaluations on questions derived from real-time chat-assisted task replication show},
    selected={true},
    pdf={https://aclanthology.org/2025.findings-acl.663.pdf},
}